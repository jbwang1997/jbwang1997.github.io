<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Towards Stable 3D Object Detection">

  <meta name="keywords" content="3D Object Detection, Stability">

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Towards Stable 3D Object Detection</title>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">


  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/interactive_figures.css">
  
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <!-- <script src="./static/js/index.js"></script> -->

  <script src="https://kit.fontawesome.com/e8d9e5563c.js" crossorigin="anonymous"></script>

  <script>
    document.addEventListener("DOMContentLoaded", function(event) { 
        //do work
      
      const copyButtonLabel = "Copy BibTex";

      // use a class selector if available
      let blocks = document.querySelectorAll("pre");
      console.log(blocks)
      
      blocks.forEach((block) => {
        // only add button if browser supports Clipboard API
        if (navigator.clipboard) {
          let button = document.createElement("button");
      
          button.innerText = copyButtonLabel;
          button.classList.add("btn");
          button.classList.add("btn-primary");
          button.style.alignContent = "center";
          button.style.textAlign = "center";
          block.parentElement.appendChild(button);
      
          button.addEventListener("click", async () => {
            await copyCode(block, button);
          });
        }
      });
      
      async function copyCode(block, button) {
        let code = block.querySelector("code");
        let text = code.innerText;
      
        await navigator.clipboard.writeText(text);
      
        // visual feedback that task is completed
        button.innerText = "BibTex Copied!";
      
        setTimeout(() => {
          button.innerText = copyButtonLabel;
        }, 1000);
      }
  });
  </script>

  <style>

    pre[class*="language-"] {
      position: relative;
      overflow: auto;
    
      /* make space  */
      margin: 5px 0;
      padding: 1.75rem 0 1.75rem 1rem;
      border-radius: 10px;
    }
    
    pre[class*="language-"] button {
      position: absolute;
      top: 5px;
      right: 5px;
    
      font-size: 0.9rem;
      padding: 0.15rem;
     
    
      border: ridge 1px;
      border-radius: 5px;
      text-shadow: #c4c4c4 0 0 2px;
    }
    
    pre[class*="language-"] button:hover {
      cursor: pointer;  
    }
  </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Towards Stable 3D Object Detection</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jbwang1997.github.io/">Jiabao Wang<sup>1</sup>*</a>,  </span>
            <span class="author-block">
              <a href="https://irvingmeng.github.io/">Qiang Meng<sup>2</sup>*</a>,  </span>
            <span class="author-block">
              Guochao Liu<sup>2</sup>, </span>
            <span class="author-block">
              Liujiang Yan<sup>2</sup>, </span>
            <span class="author-block">
              Ke Wang<sup>2</sup>, </span>
            <span class="author-block">
              <a href="https://mmcheng.net/cmm/">Ming-Ming Cheng<sup>1,3</sup></a>,  </span>
            <span class="author-block">
              <a href="https://houqb.github.io/">Qibin Hou<sup>1,3</sup>#</a> </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> VCIP, College of Computer Science, Nankai University</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>2</sup> KargoBot Inc., China</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>3</sup> NKIARI, Shenzhen Futian</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Paper Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/xxx.xxxxx" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/jbwang1997/StabilityIndex"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In autonomous driving, the temporal stability of 3D object detection greatly impacts the driving safety.
            However, the detection stability cannot be accessed by existing metrics such as mAP and MOTA, and consequently is less explored by the community.
            To bridge this gap, this work proposes <b>Stability Index (SI)</b>, a new metric that can comprehensively evaluate the stability of 3D detectors in terms of confidence, box localization, extent, and heading.
            By benchmarking state-of-the-art object detectors on the Waymo Open Dataset, SI reveals interesting properties of object stability that have not been previously discovered by other metrics.
            To help models improve their stability, we further introduce a general and effective training strategy, called <b>Prediction Consistency Learning (PCL)</b>.
            PCL essentially encourages the prediction consistency of the same objects under different timestamps and augmentations, leading to enhanced detection stability. 
            Furthermore, we examine the effectiveness of PCL with the widely-used CenterPoint, and achieve a remarkable SI of 86.00 for vehicle class, surpassing the baseline by 5.48.
            We hope our work could serve as a reliable baseline and draw the community's attention to this crucial issue in 3D object detection.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column ">
        <div class="content" style="text-align:center;">

          <h2 class="title is-3 has-text-centered">ðŸš€ Methodology</h2>
          <h2 class="title is-4"> Prediction Consistency Learning </h2>
          <img src="images/structure.png" class="" style="width: 90%;margin-bottom: 2%;">
          
          <p style="text-align:center;" class="has-text-justified">
            Most SOTA depth estimation architectures are bottlenecked by the resolution capabilities of their backbone, leading to blur depth predictions. For instance, ZoeDepth processes an input resolution of 384x512, VPD manages 480x480, and AiT is designed for 384x512. These figures pale in comparison to the resolutions offered by modern consumer cameras, such as the 45 Megapixel Canon EOS R5, the widely available 8K televisions, and even mobile devices like the iPhone 15, which boasts a 12MP Ultra Wide lens.
          </p>
          <p style="text-align:center;" class="has-text-justified">
            In this work, we make effort to addresses the challenge of metric single image depth estimation for high-resolution inputs. We propose an end-to-end tile-based framework to achieve our goal.
            It includes a <b>(1)</b> Coarse Network to provide global scale-aware estimation based on whole-image inpus, whereas high-frequency details are lost at the cost of global consistency, a <b>(2)</b> Fine Network to achieve patch-wise fine depth prediction with rich details, particularly at boundaries and intricate structure, but scale potentially inconsistent with the actual scene due to the segment property and lack of global information, and a <b>(3)</b> Guided Fusion Network with Global-to-Local (G2L) module to combine the best of two worlds.
            We also propose the consistency-aware training and inference strategy to ensure patch-wise prediction consistency.
          </p>
        </div>
        
      </div>
    </div>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content" style="text-align:center;">
          <h2 class="title is-3 has-text-centered">ðŸ”¥ Method</h2>
          <p style="text-align:center;" class="has-text-justified">
          We propose to adopt a neural network to fuse the coarse and fine depth maps, which outperforms the traditional post-optimization strategies.
          While it could be implemented by simply learning a pix2pix U-Net, our key idea is to exploit the multi-scale features from the Coarse Network and Fine Network. 
          We use two main components to achieve this transfer - the Global-to-Local Module (G2L) and the Guided Fusion Network.
          </p>
          <img src="images/fusion_net.jpg" class="" style="width: 80%;margin-bottom: 2%;">
          <h2 class="title is-4"> Global-to-Local Module and Guided Fusion Network </h2>
          <p style="text-align:center;" class="has-text-justified">
          While the key insight of G2L is to apply the global-wise self attention for each-level feature from Coarse Network to ensemble crucial information for patch-wise scale-consistent prediction, 
          we adopt the Swin Transformer Layer (STL) to preserve the global context while simultaneously alleviating GPU memory concerns.
          </p>
          <p style="text-align:center;" class="has-text-justified">
          The Guided Fusion Network follows the U-Net design. The input comprises a concatenated ensemble of the cropped original image, the corresponding cropped coarse depth estimations from Coarse Network, and fine depth estimations from Fine Network.
          After a lightweight encoder, we inject the guidance features to the skip connections and decoder layers.
          </p>

          <h2 class="title is-4">Consistency-Aware Training and Inference</h2>
          <video style="width: 80%; margin-bottom: 2%;" controls>
            <source src="images/cai.mp4" type=video/mp4>
          </video>

          <p style="text-align:center;" class="has-text-justified">
          While our Guided Fusion Network with G2L makes scale-aware predictions, boundary inconsistencies still exist. Recognizing this gap, we introduce Consistency-Aware Training (CAT) and Inference (CAI) to ensure patch-wise depth prediction consistency.

          Our methodology is based on the intuitive idea that overlapping regions between cropped patches from the same image should ideally produce consistent feature representations and depth predictions. We impose an \( L_2 \) loss on the overlapping regions of both the extracted image features and the depth predictions. While the idea of constraining the depth values is quite intuitive, the good results mainly stem from constraining the features.

          During the inference processing of patches, the updated depth is concatenated with the cropped image and coarse depth map, as the input to our guided fusion network. This dynamic updating, coupled with a running mean, engenders a local ensemble approach, incrementally refining the depth estimations on the fly. This strategy alleviates the inconsistency and further boost the prediction accuracy.
          </p>
      </div>
    </div>
  </div>
</section>


<section class="section" id="dataset-browser">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <!-- Visual Effects. -->
        <div class="column">
          <div class="content" style="text-align:center;">
            <h2 class="title is-3 has-text-centered">ðŸš€ Qualitative Results</h2>
            <img src="images/u4k.jpg" class="" style="width: 100%;margin-bottom: 2%;">
            <p style="text-align:center;" class="has-text-justified">
            <b>Qualitative results on UnrealStereo4K (first two rows) and MVS-Synth (last two rows). Left to Right:</b> Input, BoostingDepth[27], Graph-DGSR[8], PatchFusion (Ours), GT.
            </p>
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3  has-text-centered">ðŸ”¥ Citation</h2>
            <div class="language-css">
            <pre style="">
<code>
@article{li2023patchfusion,
  title={PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation}, 
  author={Zhenyu Li and Shariq Farooq Bhat and Peter Wonka},
  year={2023},
  eprint={2312.02284},
  archivePrefix={arXiv},
  primaryClass={cs.CV}}</code></pre>
            
          </div>
        </div>
      </div>
    </div>

  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is based on the <a href="https://nerfies.github.io/">Nerfies website template</a>, which is licensed under a
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<script src="./static/js/interactive_figures.js"></script>
</body>
</html>